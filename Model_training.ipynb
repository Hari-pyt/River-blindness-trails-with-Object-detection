{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3           # For interacting with S3\n",
    "import pandas as pd\n",
    "import sys             # Python system library needed to load custom functions\n",
    "import json\n",
    "# Imports to run Sagemaker training jobs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import Session\n",
    "\n",
    "import matplotlib.pyplot as plt  # Used for plotting\n",
    "import mmcv  # Object detection framework\n",
    "import os  # Interaction with the file system\n",
    "import pandas as pd  # Home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import sys  # Python system library needed to load custom functions\n",
    "\n",
    "from matplotlib.patches import Rectangle  # Allows drawing the bounding boxes of the worm sections\n",
    "from mmcv import Config  # Loading and accessing MMDetection configuration files\n",
    "from mmdet.apis import inference_detector, init_detector, train_detector, set_random_seed  # Part of the MMDetection framework\n",
    "from mmdet.datasets import build_dataset  # Part of the MMDetection framework\n",
    "from mmdet.models import build_detector  # Part of the MMDetection framework\n",
    "\n",
    "from PIL import Image  # For loading image files\n",
    "from tqdm import tqdm  # for timing a for loop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mmdet.utils import AvoidCUDAOOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')  # Add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  # The name of the S3 bucket that contains the training data\n",
    "from detection_util import create_predictions\n",
    "from gdsc_util import download_and_extract_model, set_up_logging, extract_hyperparams,create_encrypted_bucket, PROJECT_DIR, upload_to_s3,\\\n",
    "load_sections_df\n",
    "from PredictionEvaluator import PredictionEvaluator\n",
    "#from mmdet.apis import inference_detector, init_detector, train_detector, set_random_seed\n",
    "set_up_logging()  # Sets up logging to console and .log\n",
    "\n",
    "from detection_util import create_predictions\n",
    "from gdsc_score import get_leaderboard_score\n",
    "from gdsc_util import download_directory, download_file, load_sections_df, set_up_logging, PROJECT_DIR\n",
    "from PredictionEvaluator import PredictionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment name\n",
    "entry_point = 'training_config.py'\n",
    "import training_config as exp\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=DEFAULT_REGION)\n",
    "sess = Session(sagemaker_client=sm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data in the input channel will be copied to training job container\n",
    "input_channels = {    \n",
    "    \"train\": f\"s3://gdsc5/data\",\n",
    "}\n",
    "# we need to create our own s3 bucket if it doesn't exist yet:\n",
    "s3_output_location = f\"s3://gdsc5/train_runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import boto3 # uploading the data to my own bucket\n",
    "#client = boto3.client('s3')\n",
    "# upload_to_s3('../data/actual_train.csv', 'data/actual_train.csv', 'gdsc5')\n",
    "# upload_to_s3('../data/actual_test.csv', 'data/actual_test.csv', 'gdsc5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting hyper parameters for training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reloading the module after small changes\n",
    "import importlib\n",
    "importlib.reload(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config to check for any errors\n",
    "data_folder = str(PROJECT_DIR / 'data')\n",
    "cfg, base_file = exp.load_config(data_folder)\n",
    "hyperparameters = extract_hyperparams(entry_point) # custom function to parse the training script and extract config\n",
    "hyperparameters['base_file'] = base_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg['data']['test']['pipeline'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the config file \n",
    "# text_file = open(\"../src/custom/exp_4a1_conf.py\", \"w\")\n",
    "# text_file.write(cfg.pretty_text)\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "datasets\n",
    "\n",
    "\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "# model.CLASSES = datasets[0].CLASSES  # Add an attribute for visualization convenience\n",
    "\n",
    "# train_detector(model, datasets, cfg, validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to specify which metrics we want Sagemaker to automatically track. For this we need to setup [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) that will be applied on the logs.\n",
    "The corresponding values will then be stored and made visible in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for coco dataset format evaluations\n",
    "# # Output format is:\n",
    "# # INFO:mmdet:Epoch [4][50/497]#011lr: 5.000e-03, eta: 0:10:23, time: 1.638, data_time: 1.433, memory: 1863, loss_rpn_cls: 0.0897, loss_rpn_bbox: 0.0781, loss_cls: 0.2336, acc: 90.8691, loss_bbox: 0.3404, loss: 0.7418\n",
    "# metrics = [\n",
    "# {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "# {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "# {\"Name\": \"val:AP 0.75\", \"Regex\": \"Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=400 ] = ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"val:AR 0.75\", \"Regex\": \"Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=400 ] = ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"val:bbox\", \"Regex\": \"bbox_mAP_50: ([0-9\\.]+)\"},\n",
    "# {\"Name\": \"val:bbox\", \"Regex\": \"bbox_mAP_75: ([0-9\\.]+)\"},\n",
    "\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "    {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "    {\"Name\": \"val:mAP\", \"Regex\": \"mAP: ([0-9\\.]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=entry_point,             # This function will be called by the training job\n",
    "    source_dir=\"../src\",                 # All code in this folder will be copied over\n",
    "    image_uri=f\"954362353459.dkr.ecr.{DEFAULT_REGION}.amazonaws.com/sm-training-custom:torch-1.8.1-cu111-noGPL\",\n",
    "    role=role,\n",
    "    output_path=s3_output_location,\n",
    "    container_log_level=20,             # 10=debug, 20=info\n",
    "    base_job_name=exp_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",     # a GPU instance\n",
    "    volume_size=45,\n",
    "    metric_definitions=metrics,\n",
    "   hyperparameters=hyperparameters,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created the estimator, we will need to call the .fit method to start the training job. As this might take a while, we set ```wait=False``` so our notebook will not wait for the training job to finish and we can continue working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    input_channels,\n",
    "    wait=False,           # Whether or not the notebook should wait for the job to finish. By setting it to False we can continue working while the job runs on another machine.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
