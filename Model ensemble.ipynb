{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c11a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../src')  \n",
    "from detection_util import create_predictions\n",
    "from gdsc_score import get_leaderboard_score\n",
    "from gdsc_util import download_directory, download_file, load_sections_df, set_up_logging, PROJECT_DIR\n",
    "from PredictionEvaluator import PredictionEvaluator\n",
    "pd.options.display.max_rows = 150\n",
    "from ensemble_boxes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842abdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_box(df):\n",
    "    df['x1'] = df.apply(lambda x : x['xmin']/x['width'], axis = 1)\n",
    "    df['y1'] = df.apply(lambda x : x['ymin']/x['height'], axis = 1)\n",
    "    df['x2'] = df.apply(lambda x : x['xmax']/x['width'], axis = 1)\n",
    "    df['y2'] = df.apply(lambda x : x['ymax']/x['height'], axis = 1)\n",
    "    \n",
    "    return df.copy()\n",
    "\n",
    "def de_scale_box(df):\n",
    "    df['xmin'] = df.apply(lambda x : np.round(x['xmin']*x['width']), axis = 1)\n",
    "    df['ymin'] = df.apply(lambda x : np.round(x['ymin']*x['height']), axis = 1)\n",
    "    df['xmax'] = df.apply(lambda x : np.round(x['xmax']*x['width']), axis = 1)\n",
    "    df['ymax'] = df.apply(lambda x : np.round(x['ymax']*x['height']), axis = 1)\n",
    "    \n",
    "    return df.copy()\n",
    "\n",
    "def bulid_section_id(x):\n",
    "\n",
    "    name = \"{}@{}-{}-{}-{}\"\n",
    "    return name.format(x['file_name'], x['xmin'], x['xmax'],  x['ymin'], x['ymax'])  \n",
    "\n",
    "def get_score(test):\n",
    "\n",
    "    # finding the best confidence score\n",
    "    ground_truth = load_sections_df(f'actual_test.csv')\n",
    "    confidence_score = np.arange(0.35,0.7,0.05)\n",
    "    score_dict ={}\n",
    "    score_df = pd.DataFrame(columns = ['confidence', 'board_score', 'ground_labels_count', \n",
    "                                       'actual_count_worms', 'pred_count_worms' ])\n",
    "\n",
    "\n",
    "    for i in tqdm(confidence_score):\n",
    "\n",
    "        cf_df = test[test['detection_score'] >=i]\n",
    "\n",
    "        evaluator = PredictionEvaluator(ground_truth)\n",
    "        thresholds = [0.5, 0.6, 0.7]\n",
    "        score_json = get_leaderboard_score(cf_df, thresholds, evaluator,detailed_evaluation = False )\n",
    "        score_dict[i] = score_json\n",
    "        score_df.loc[len(score_df)] = [i, score_json['score'], ground_truth.shape[0], test.shape[0], cf_df.shape[0] ]\n",
    "\n",
    "    return score_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ac2468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_a = pd.read_csv('../Model_evaluation/preds/exp_4_a_24_test.csv')\n",
    "model_a = pd.read_csv('Ensemble_4a_4b.csv')\n",
    "Model_b = pd.read_csv('../Model_evaluation/preds/exp_5_b_20_test.csv')\n",
    "actual_data = pd.read_csv('actual_test.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a9e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data[['file_name', 'height', 'width' ]].drop_duplicates().copy()\n",
    "model_a = pd.merge(model_a, actual_data, on = 'file_name')\n",
    "model_b = pd.merge(Model_b, actual_data, on = 'file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b14c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'width' not in model_a.columns:\n",
    "    model_a = model_a.rename(columns = {'width_x' : 'width' , 'height_x' : 'height' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f364562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = scale_box(model_a)\n",
    "model_b = scale_box(model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a9e61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_lst = list(model_a[['x1', 'y1', 'x2', 'y2']].values)\n",
    "model_b_lst = list(model_b[['x1', 'y1', 'x2', 'y2']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7275348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_holder = []\n",
    "all_images = actual_data['file_name'].unique()\n",
    "for img in all_images:\n",
    "    \n",
    "    temp_df_a = model_a[model_a['file_name'] == img]\n",
    "    temp_df_b = model_b[model_b['file_name'] == img]\n",
    "    \n",
    "    # sanity check\n",
    "    if not (temp_df_a['height'].values[0] ==  temp_df_b['height'].values[0]) and \\\n",
    "    (temp_df_a['width'].values[0] ==  temp_df_b['width'].values[0]):\n",
    "        raise ValueError(\"something went wrong\")\n",
    "    \n",
    "    \n",
    "    lst_temp_a = list(temp_df_a[['x1', 'y1', 'x2', 'y2']].values)\n",
    "    lst_temp_b = list(temp_df_b[['x1', 'y1', 'x2', 'y2']].values)\n",
    "    \n",
    "    cf_a = temp_df_a['detection_score'].values\n",
    "    cf_b = temp_df_b['detection_score'].values\n",
    "    \n",
    "    labels_a = np.repeat(1, temp_df_a.shape[0])\n",
    "    labels_b = np.repeat(1, temp_df_b.shape[0])\n",
    "\n",
    "    \n",
    "    boxes_list = [lst_temp_a, lst_temp_b]\n",
    "    scores_list = [cf_a, cf_b]\n",
    "    labels_list = [labels_a, labels_b]\n",
    "    \n",
    "    weights = [1, 2]\n",
    "    iou_thr = 0.5\n",
    "    skip_box_thr = 0.0001\n",
    "    sigma = 0.1\n",
    "    \n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, \n",
    "                                                  weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    \n",
    "    # rebuilding df\n",
    "    rebuild_df = pd.DataFrame(boxes, columns = ['xmin', 'ymin', 'xmax', 'ymax'])\n",
    "    rebuild_df['width'] = temp_df_a['width'].values[0]\n",
    "    rebuild_df['height'] = temp_df_a['height'].values[0]\n",
    "    rebuild_df['detection_score'] = scores\n",
    "    rebuild_df['file_name'] = img\n",
    "    rebuild_df = de_scale_box(rebuild_df)\n",
    "    rebuild_df['section_id'] = rebuild_df.apply(lambda x : bulid_section_id(x), axis=1)\n",
    "    df_holder.append(rebuild_df)\n",
    "    \n",
    "ensemble_df = pd.concat(df_holder, axis = 0)\n",
    "    \n",
    "ensemble_df.to_csv('temp.csv', index= False)\n",
    "ensemble_df = pd.read_csv('temp.csv')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa91b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:15<00:00, 10.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>board_score</th>\n",
       "      <th>ground_labels_count</th>\n",
       "      <th>actual_count_worms</th>\n",
       "      <th>pred_count_worms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35</td>\n",
       "      <td>236.60</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>238.74</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.45</td>\n",
       "      <td>240.64</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>241.88</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.55</td>\n",
       "      <td>243.21</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.60</td>\n",
       "      <td>243.73</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>6061.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.65</td>\n",
       "      <td>243.19</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>5925.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   confidence  board_score  ground_labels_count  actual_count_worms  \\\n",
       "0        0.35       236.60               6220.0              9594.0   \n",
       "1        0.40       238.74               6220.0              9594.0   \n",
       "2        0.45       240.64               6220.0              9594.0   \n",
       "3        0.50       241.88               6220.0              9594.0   \n",
       "4        0.55       243.21               6220.0              9594.0   \n",
       "5        0.60       243.73               6220.0              9594.0   \n",
       "6        0.65       243.19               6220.0              9594.0   \n",
       "\n",
       "   pred_count_worms  \n",
       "0            6662.0  \n",
       "1            6523.0  \n",
       "2            6417.0  \n",
       "3            6300.0  \n",
       "4            6181.0  \n",
       "5            6061.0  \n",
       "6            5925.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(ensemble_df) # 4A + 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746edaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 4A + 4B\n",
    "# ensemble_df.to_csv('Ensemble_4a_4b.csv', index =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fba4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.to_csv('Ensemble_4a_4b_5b.csv', index =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfe35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
